#!/usr/bin/env python2
# -*- coding: utf-8 -*
import argparse
import glob
import time
import os
import sys
sys.path.append(os.getcwd())
import re
import importlib
from metis.CondorTask import CondorTask
from metis.StatsParser import StatsParser
from metis.Utils import good_sites

def check(samples, ceph_dir, tag, files_per_job):
    for sample in samples:
        name = sample.get_datasetname()
        dir_name = name[1:].replace("/", "_")
        sample_dir = "{0}/{1}/{2}_{1}".format(ceph_dir, tag, dir_name)
        n_results = len(glob.glob("{}/output*.root".format(sample_dir)))

        n_files = len(sample.get_files())
        n_jobs = n_files//files_per_job + (n_files%files_per_job > 0)

        if n_results != n_jobs:
            print("Missing {0} outputs for {1}".format(n_jobs - n_results, name))
        else:
            print("All outputs found for {}".format(name))

cli = argparse.ArgumentParser(description="Submit condor jobs")
cli.add_argument(
    "study", type=str,
    help="Name of the study to run"
)
cli.add_argument(
    "-y", "--yes",
    action="store_true",
    help="Skip any (y/n) prompts"
)
cli.add_argument(
    "--debug",
    action="store_true",
    help="Run in debug mode"
)
cli.add_argument(
    "--python2",
    action="store_true",
    help="Use Python2 to run nanoAOD-tools on the condor worker node"
)
cli.add_argument(
    "--name", type=str, required=True,
    help="full path of directory to write skims to"
)
cli.add_argument(
    "--samples", type=str, required=True,
    help="sample to import"
)
cli.add_argument(
    "--tag", type=str, required=True,
    help="Unique tag for submissions"
)
cli.add_argument(
    "--filter", type=str,
    help="Regex filter for excluding matching datasets"
)
cli.add_argument(
    "--xrootd_protocol", type=str, default="root",
    help="File transfer protocol for XRootD to use for reading files"
)
cli.add_argument(
    "--xrootd_host", type=str, default="xcache-redirector.t2.ucsd.edu:2042",
    help="<IP>:<port> of desired XRootD host for reading files"
)
cli.add_argument(
    "--sites", type=str, nargs="*", default=good_sites,
    help="Space-separated list of T2 sites"
)
cli.add_argument(
    "--n_monit_hrs", type=int, default=48,
    help="Number of hours to run Metis for"
)
cli.add_argument(
    "--max_jobs", type=int, default=0,
    help="Maximum number of jobs per sample"
)
cli.add_argument(
    "--n_inputs", type=int, default=3,
    help="Number of input files per job"
)
cli.add_argument(
    "--check",
    action="store_true",
    help="Manually check how many outputs are missing for each job"
)
args = cli.parse_args()

samples_module, samples_obj = args.samples.rsplit(".", 1)
samples = getattr(importlib.import_module(samples_module), samples_obj)

if args.python2:
    CMSSW_VERSION="CMSSW_10_6_25",
    SCRAM_ARCH="slc7_amd64_gcc700",
else:
    CMSSW_VERSION = "CMSSW_12_2_0"
    SCRAM_ARCH = "slc7_amd64_gcc900"

PACKAGE = "packages/{}.tar.gz".format(args.study)
if not os.path.isfile(PACKAGE):
    print("ERROR: {} does not exist! Run bin/make_package first".format(PACKAGE))
    exit()

# Assemble condor_submit parameters
condor_submit_params = {
    "sites": ",".join(args.sites), 
    "classads": [
        ["XRootDHost", args.xrootd_host], 
        ["XRootDProtocol", args.xrootd_protocol],
        ["StudyName", args.study],
        ["JobBatchName", "{}_{}".format(args.name, args.tag)]
    ],
}
if args.python2:
    condor_submit_params["classads"].append(["UsePython2", "true"])

additional_input_files = []
token_file = ""
if "BEARER_TOKEN" in os.environ.keys():
    token_file = "/tmp/bt_u{}".format(os.getuid())
    with open(token_file, "w") as f_out:
        f_out.write(os.environ["BEARER_TOKEN"])
elif "BEARER_TOKEN_FILE" in os.environ.keys():
    token_file = os.environ["BEARER_TOKEN_FILE"]
elif "XDG_RUNTIME_DIR" in os.environ.keys():
    token_file = "{0}/bt_u{1}".format(os.environ["XDG_RUNTIME_DIR"], os.getuid())
    if not os.path.isfile(token_file):
        token_file = ""

if token_file == "" and os.path.isfile("/tmp/bt_u{}".format(os.getuid())):
    token_file = "/tmp/bt_u{}".format(os.getuid())

if token_file != "" and os.path.isfile(token_file):
    print("Found token file: {}".format(token_file))
    condor_submit_params["extra_commands"] = []
    # condor_submit_params["extra_commands"].append(["use_scitokens", "True"])
    # condor_submit_params["extra_commands"].append(["scitokens_file", token_file])
    condor_submit_params["extra_commands"].append(["encrypt_input_files", token_file])
    condor_submit_params["classads"].append(["SciTokenFilename", token_file.split("/")[-1]])
    additional_input_files.append(token_file)

if args.filter:
    filter_re = re.compile(args.filter)
    samples = [s for s in samples if filter_re.match(s.get_datasetname())]
    print("Filter '{}' matched the following datasets:".format(args.filter))
    print("\n".join([s.get_datasetname() for s in samples]))
    if not args.yes:
        resp = ""
        while resp != "y" and resp != "n":
            resp = raw_input("Submit condor jobs for these samples? y/n: ").lower()
        if resp == "n":
            exit()

if args.check:
    check(samples, args.name, args.tag, args.n_inputs)
    exit()

if args.debug:
    samples = samples[:1]
    max_jobs = 1
else:
    max_jobs = args.max_jobs

total_summary = {}
n_updates = max(args.n_monit_hrs*2, 1)
for update_i in range(n_updates): # update every 30 mins
    if update_i > 0:
        time.sleep(30*60) # Wait 30 minutes
    # Collect tasks
    tasks = []
    for sample in samples:
        task = CondorTask(
            sample=sample,
            files_per_output=args.n_inputs,
            output_name="output.root",
            tag=args.tag,
            condor_submit_params=condor_submit_params,
            cmssw_version=CMSSW_VERSION,
            scram_arch=SCRAM_ARCH,
            input_executable="scripts/condor_executable.sh",          # your condor executable here
            additional_input_files=additional_input_files,
            tarfile=PACKAGE,                                   # your tarfile with assorted goodies here
            special_dir="{0}/{1}".format(args.name, args.tag), # output files into /ceph/cms/store/<user>/<special_dir>
            max_jobs=max_jobs
        )
        tasks.append(task)
    # Set task summary
    for task in tasks:
        if not task.complete():
            task.process()
        total_summary[task.get_sample().get_datasetname()] = task.get_task_summary()
    if args.debug:
        break
    # Update monitoring GUI
    StatsParser(data=total_summary, webdir="~/public_html/{}_metis".format(args.name.lower())).do()
